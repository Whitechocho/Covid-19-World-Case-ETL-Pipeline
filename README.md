# Covid-19-World-Case-ETL-Pipeline

In this project, I took on the challenge of processing and analyzing the vast amounts of data related to the global spread of Covid-19. I utilized my skills as a data engineer to design and develop an end-to-end ETL (Extract, Transform, Load) pipeline that could handle the large amounts of data and make it accessible for analysis.

### 1. Extract Process: 
The extract phase of the pipeline involved obtaining data related to the Covid-19 cases from a publicly available API. The data was in raw form in .JSON formats and required further processing in order to be usable for analysis. I was able to successfully extract this data and prepare it for the next stage of the pipeline.

### 2. Transform Process: 
The transform phase was where I was able to demonstrate my expertise in data processing and transformation. I transformed the raw data into multiple tables, breaking down the original data into smaller, more manageable chunks and converting it into a structured format. This allowed for a much easier and efficient analysis of the data, as the smaller chunks were much easier to manipulate and understand. Additionally, I created an Entity Relationship Diagram (ERD) to visualize the relationships between the various tables, further assisting in the analysis of the data.

<p align="center">
  <img src="https://user-images.githubusercontent.com/56055678/218764986-298dad86-398a-4dc9-bdda-a8958d2ee6d5.png" width="700">
</p>

### 3. Load Process: 
The load phase involved loading the transformed data into AWS S3, a cloud storage service. This was a crucial step as it made the data accessible for further analysis and ensured that it was stored in a secure and scalable manner. I utilized my knowledge of cloud computing to ensure that the data was stored in an efficient and cost-effective way, making it accessible to authorized users and systems.

To further illustrate my work on the Covid-19-World-Case-ETL-Pipeline, I have included a visual representation of the ETL schemes I used to extract, transform, and load the data related to the global spread of Covid-19. By implementing the ETL pipeline, I could extract data from a publicly available API, transform the raw data into multiple tables, and load the transformed data into AWS S3. The loaded data was then accessible for further analysis, and I utilized my expertise in uploading the data into PostgreSQL for analytical purposes. This demonstrated my ability to extract and process data and store and analyze it for valuable insights and decision-making.

<p align="center">
  <img src="https://user-images.githubusercontent.com/56055678/218787309-d193e28c-4a6b-4042-8b20-05be42c639d0.png" width="700">
</p>

This project allowed me to further enhance my data engineering skills, particularly in the areas of data extraction, transformation, and loading. As the importance of data in decision making continues to grow, I am confident that these skills will be highly-valuable in my future endeavors. I am eager to apply my knowledge and expertise to new and challenging projects, and I am confident that I have the skills and experience needed to succeed.
